---
title: Egress FQDN Filtering with Aviatrix
author: admin
type: post
date: -001-11-30T00:00:00+00:00
draft: true
url: /?p=452
categories:
  - Aviatrix
  - AWS
  - Azure
  - Cloud
  - NAT

---
Securing access to your workloads running in the public cloud is a critical practice that enterprises should follow when moving to the cloud. That being said, what if those workloads need access to the public internet to run specific tasks? This could be installing a python package via PIP or downloading OS updates through your Linux package manager of choice. Do you want this traffic backhauled to the data centre, consuming bandwidth on your hybrid connectivity and internet circuits? With the workload already running in the public cloud, there&#8217;s an easier way to let this traffic egress directly to the public internet while also maintaining control and visibility around what sites are accessed by these workloads. But even if you allowed direct internet access via a native cloud NAT gateway, how would you secure that traffic? Yes, things like security groups can be configured, but that&#8217;s all done at the IP address level, and as well all know&#8230; domains are often mapped to many different IP addresses. Let&#8217;s look at Aviatrix Egress FQDN Filtering. 



## Functionality and Components {.wp-block-heading}

[Overview of Egress FQDN features]

### Discovery Mode {.wp-block-heading}

Let&#8217;s say you want to enable Egress FQDN filtering on your Aviatrix environment&#8230; but you&#8217;re a little unsure if you know ALL the domains your workloads need access to. Because app developers always know the connectivity requirements for every app&#8230; right? 

Enabling Egress FQDN Discovery can be a great starting point to help administrators get some insight into what connectivity requirements are based on real-time traffic. Enabling this feature does the following:

  1. Enables the Aviatrix Gateway for SNAT (essentially providing the functionality of a NAT Gateway
  2. Inserts a 0.0.0.0/0 route into all private route tables associated with the VPC/VNet pointed towards the ENI of the Aviatrix Gateway
  3. All egress traffic is PERMITTED
  4. All discovered FQDNs are logged on a per-gateway basis

Note: When changing from Discovery Mode to Enforcement mode, the discovery logs will be wiped, so it&#8217;s a good idea to save a copy of the logs before switching.

To enable this feature you can simply navigate to the Egress Control section within the Aviatrix Controller, select the appropriate gateway under Egress FQDN Discovery, and select &#8220;Start.&#8221;<figure class="wp-block-image size-large">

<img decoding="async" src="https://thecontrolplane.files.wordpress.com/2023/01/enablediscovery.png?w=1024" alt="" class="wp-image-566" /> </figure> 



Let&#8217;s take a quick look at what happens when discovery mode is enabled on apse2-demo-spoke&#8230;

Since this is a spoke gateway attached to an Aviatrix Transit, we can see that the private route tables currently have entries for RFC1918 pointed toward the ENI of the spoke gateway, as expected.<figure class="wp-block-image size-large">

<img decoding="async" src="https://thecontrolplane.files.wordpress.com/2023/01/rtbeforediscovery.png?w=1024" alt="" class="wp-image-568" /> </figure> 

We can see that our demo machine that resides in this subnet currently has no internet connectivity either:<figure class="wp-block-image size-large">

<img decoding="async" src="https://thecontrolplane.files.wordpress.com/2023/01/curlbeforediscovery.png?w=968" alt="" class="wp-image-574" /> </figure> 



Once we enable the spoke for discovery, we can see a success message stating that route tables have now been updated to direct traffic to the gateway.<figure class="wp-block-image size-large">

<img decoding="async" src="https://thecontrolplane.files.wordpress.com/2023/01/enablediscoverymsg.png?w=658" alt="" class="wp-image-570" /> </figure> 

We now see the private route table has been update with an additional entry for a default route also pointed to the Aviatrix spoke gateway.<figure class="wp-block-image size-large">

<img decoding="async" src="https://thecontrolplane.files.wordpress.com/2023/01/rtafterdiscovery.png?w=1024" alt="" class="wp-image-572" /> </figure> 

When we run similar connectivity tests on the demo VM, we now see successful connectivity.<figure class="wp-block-image size-large">

<img decoding="async" src="https://thecontrolplane.files.wordpress.com/2023/01/curlafterdiscovery.png?w=1024" alt="" class="wp-image-576" /> </figure> 

Additionally, if we check the discovery logs on the gateway, we can see all the domains we tested with have been discovered in this process:<figure class="wp-block-image size-large">

<img decoding="async" src="https://thecontrolplane.files.wordpress.com/2023/01/discoverylog.png?w=1024" alt="" class="wp-image-578" /> </figure> 



### Enforcement Mode {.wp-block-heading}

## Topology {.wp-block-heading}

We have a simple topology with a pair of spokes and transits built in both AWS and Azure. We also have test VMs within the private subnets of each CSP for testing. <figure class="wp-block-image size-large">

<img decoding="async" src="https://thecontrolplane.files.wordpress.com/2023/01/topology-1.png?w=1024" alt="" class="wp-image-590" /> </figure> 

Today we&#8217;ll walk through 3 scenarios in which we could use Aviatrix Egress FQDN filtering to allow our private workloads to talk to an approved list of FQDNs (domains). 



## 1. Egress FQDN Direct from Spoke VPC/VNet {.wp-block-heading}

Since we already have Aviatrix Spoke gateways deployed into each spoke VPC/VNet that leverage the use of public IP addressing, this is the simplest way to grant this access. 



## 2. Centralised Egress FQDN Filtering on Spoke {.wp-block-heading}



### Centralized Egress via Dedicated Spoke {.wp-block-heading}

  * Show up to 3 AZ



## 3. centralised Egress FQDN Filtering &#8211; PSF Gateway {.wp-block-heading}